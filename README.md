 # LVM-Demo: Pixtral 12B Multimodal AI with watsonx 🚀🖼️
  AICTE - IBM Internship on AI &amp; Cloud Technologies by Edunet Foundation 


## Definition 📚
LVM-Demo showcases the use of the Pixtral 12B multimodal Large Language Model (LLM) from Mistral AI, hosted on IBM watsonx.ai. The project demonstrates image captioning and visual question answering capabilities within a Jupyter Notebook environment.

## Project Overview 📝
This tutorial walks through setup and usage of the Pixtral 12B model on IBM’s watsonx.ai platform. It covers environment and runtime configuration, API key management, image encoding, and model inference for both Pixtral 12B and Llama 11B vision-instruct models. The project is part of the AICTE - IBM Internship on AI & Cloud Technologies by Edunet Foundation.

## How to Use ⚙️
1. **IBM Cloud Setup:** Create/activate your IBM Cloud account and set up a watsonx.ai project (Pixtral 12B currently available in Europe, Frankfurt & London).
2. **Jupyter Notebook:** Create a new or upload the provided notebook.
3. **Install Dependencies:** Run notebook cells to install `ibm_watsonx_ai`.
4. **Run Notebook:** Execute all cells to encode images, query models, and display results.

## Features ✨
- Multimodal AI: Processes images and text together.
- High-resolution document, chart, and graph understanding.
- Visual question answering and image captioning.
- Response comparison between Pixtral 12B and Llama 11B vision-instruct models.
- Simple image encoding workflow (Base64).

## Example Workflow 🔄
1. Fetch image via URL and encode as Base64.
2. Submit an image and text query to Pixtral 12B via watsonx.ai API.
3. Display AI-generated response.
4. Repeat steps for Llama 11B model for comparison.

## Technologies Used 🛠️
- Mistral AI Pixtral 12B
- IBM watsonx.ai
- `ibm_watsonx_ai` Python library
- Jupyter Notebook

## References 🔗
- [IBM watsonx.ai Documentation](https://www.ibm.com/products/watsonx-ai)
